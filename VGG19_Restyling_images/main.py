# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AH9QdPF0OdqH9J7bq6ISXEr6CaRFpPXQ
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from google.colab import files
from io import BytesIO
import PIL
from PIL import Image

img = list(files.upload().values())[0]
img_style = list(files.upload().values())[0]

img = Image.open(BytesIO(img))
img_style = Image.open(BytesIO(img_style))
img = img.resize((224,224), PIL.Image.NEAREST)
img_style = img_style.resize((224,224), PIL.Image.NEAREST)
x_img = keras.applications.vgg19.preprocess_input(np.expand_dims(img, axis = 0))
x_style = keras.applications.vgg19.preprocess_input(np.expand_dims(img_style, axis = 0))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.subplot(1, 2, 2)
plt.imshow(img_style)
plt.show()

def deprocess_img(processed_img):
  x = processed_img.copy()
  if len(x.shape) == 4:
    x = np.squeeze(x, 0)
  assert len(x.shape) == 3, ("must be [1, height, width, channel] or [height, width, channel]")
  if len(x.shape) != 3:
    raise ValueError('Invalid Input')
  x[:, :, 0] += 103.939
  x[:, :, 1] += 116.779
  x[:, :, 2] += 123.68
  x = x[:, :, ::-1]
  x = np.clip(x, 0, 255).astype('uint8')
  return x

content_layers = ['block5_conv2']

style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1',
                'block4_conv1',
                'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

vgg = keras.applications.vgg19.VGG19(include_top = False, weights = 'imagenet')
vgg.trainable = False

style_outputs = [vgg.get_layer(name).output for name in style_layers]
content_outputs = [vgg.get_layer(name).output for name in content_layers]
model_outputs = style_outputs + content_outputs


model = keras.models.Model(vgg.input, model_outputs)
for layer in model.layers:
  layer.trainable = False

def get_feature_representations(model):
  style_outputs = model(x_style)
  content_outputs = model(x_img)

  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]
  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]
  return style_features, content_features

def get_content_loss(base_content, target):
  return tf.reduce_mean(tf.square(base_content - target))

def gram_matrix(input_tensor):
  channels = int(input_tensor.shape[-1])
  a = tf.reshape(input_tensor, [-1, channels])
  n = tf.shape(a)[0]
  gram = tf.matmul(a, a, transpose_a=True)
  return gram / tf.cast(n, tf.float32)

def get_style_loss(base_style, gram_target):
  gram_style = gram_matrix(base_style)
  return tf.reduce_mean(tf.square(gram_style - gram_target))

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
  style_weight, content_weight = loss_weights

  model_outputs = model(init_image)

  style_output_features = model_outputs[:num_style_layers]
  content_output_features = model_outputs[num_style_layers:]

  style_score = 0
  content_score = 0

  weight_per_style_layer = 1.0 / float(num_style_layers)
  for target_style, comb_style in zip(gram_style_features, style_output_features):
    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)

  weight_per_content_layer = 1.0 / float(num_content_layers)
  for target_content, comb_content in zip(content_features, content_output_features):
    content_score += weight_per_content_layer * get_content_loss(comb_content[0], target_content)

  style_score *= style_weight
  content_score *= content_weight

  loss = style_score + content_score
  return loss, style_score, content_score

num_iterations = 1000
content_weight = 1
style_weight = 5000

style_features, content_features = get_feature_representations(model)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

init_image = np.copy(x_img)
init_image = tf.Variable(init_image, dtype=tf. float32)

opt = tf.compat.v1.train.AdamOptimizer(learning_rate = 2, beta1 =0.99, epsilon = 0.1)
iterations_count = 1
best_loss, best_img = float('inf'), None
loss_weights = (style_weight, content_weight)
cfg = {
  'model': model,
  'loss_weights': loss_weights,
  'init_image': init_image,
  'gram_style_features': gram_style_features,
  'content_features': content_features
}

norm_means = np.array([103.939, 116.779, 123.68])
min_vals = - norm_means
max_vals = 255 - norm_means
imgs = []

part = 0

for i in range(num_iterations):
  with tf.GradientTape() as tape:
    all_loss = compute_loss(**cfg)
  total_loss = all_loss[0]
  grads = tape.gradient(total_loss, init_image)
  loss, style_score, content_score = all_loss
  opt.apply_gradients([(grads,init_image)])
  clipped = tf.clip_by_value(init_image, min_vals, max_vals)
  init_image.assign(clipped)
  if loss < best_loss:
    best_loss = loss
    best_img = deprocess_img(init_image.numpy())
    imgs.append(best_img)
    if(i>=part):
      print(f'{(i/num_iterations)*100} %')
      part+=num_iterations*0.1

plt.imshow(best_img)
print(best_loss)

image = Image.fromarray(best_img.astype('uint8'), 'RGB')
image.save("out.jpg")
files.download("out.jpg")